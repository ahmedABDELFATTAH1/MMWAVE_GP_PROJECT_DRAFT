{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from plotly import offline\n",
    "from plotly import graph_objs as go\n",
    "from PIL import Image\n",
    "import plotly.express as px\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from skimage.filters import threshold_minimum\n",
    "import torch\n",
    "import keras\n",
    "from keras.models import Sequential,Input,Model\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "from keras.models import load_model\n",
    "from keras import regularizers\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color = px.colors.sequential.Rainbow[::-1]\n",
    "marker_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3D_threshold (exp_name ,folder_indx , depth_axis ,threshold):\n",
    "    folder = [\"3D_Experements\", \"flat_Experements\"]\n",
    "    dist = np.loadtxt(folder[folder_indx]+\"/\"+exp_name+\"_x.txt\")\n",
    "    upper_angle = np.loadtxt(folder[folder_indx]+\"/\"+exp_name+\"_y.txt\")\n",
    "    lower_angle = np.loadtxt(folder[folder_indx]+\"/\"+exp_name+\"_z.txt\")\n",
    "\n",
    "    print(np.amax(upper_angle*180/np.pi))\n",
    "    print(np.amin(upper_angle*180/np.pi))\n",
    "    print(np.amax(lower_angle*180/np.pi))\n",
    "    print(np.amin(lower_angle*180/np.pi))\n",
    "    \n",
    "\n",
    "    #x , y , z = np.array(dist)*np.cos(uAngel)*np.sin(lAngel) , np.array(dist)*np.cos(uAngel)*np.cos(lAngel) , np.array(dist)*np.sin(uAngel)\n",
    "\n",
    "    my_sample_x = np.array(dist)*np.cos(upper_angle)*np.sin(lower_angle)\n",
    "    my_sample_y =  np.array(dist)*np.cos(upper_angle)*np.cos(lower_angle)\n",
    "    my_sample_z =  np.array(dist)*np.sin(upper_angle)\n",
    "    my_sample_x = my_sample_x[~np.isnan(my_sample_x)]\n",
    "    my_sample_y = my_sample_y[~np.isnan(my_sample_y)]\n",
    "    my_sample_z = my_sample_z[~np.isnan(my_sample_z)]\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    indx = []\n",
    "    min_depth = 0\n",
    "    max_depth = 0\n",
    "    if depth_axis == 'x':\n",
    "        min_depth = np.amin(my_sample_x)\n",
    "        max_depth = threshold#(min_depth + threshold)\n",
    "        indx = my_sample_x<= threshold#(min_depth + threshold)\n",
    "        df['Depth'] = my_sample_x[indx]\n",
    "\n",
    "    elif depth_axis == 'y':\n",
    "        min_depth = np.amin(my_sample_y)\n",
    "        max_depth = threshold#(min_depth + threshold)\n",
    "        indx = my_sample_y <= threshold#(min_depth + threshold)\n",
    "#         print(my_sample_y , (min_depth + threshold))\n",
    "#         print(type(my_sample_y),type(my_sample_y[indx]))\n",
    "        df['Depth'] = my_sample_y[indx]\n",
    "\n",
    "    elif depth_axis == 'z':\n",
    "        min_depth = np.amin(my_sample_z)\n",
    "        max_depth = threshold#(min_depth + threshold)\n",
    "        indx = my_sample_z<= threshold#(min_depth + threshold)\n",
    "        df['Depth'] = my_sample_z[indx]\n",
    "        \n",
    "\n",
    "    else:\n",
    "        print(\"invalid depth axis\")\n",
    "        return\n",
    "    df['X (mm)'] = my_sample_x[indx]\n",
    "    df['Y (mm)'] = my_sample_y[indx]\n",
    "    df['Z (mm)'] = my_sample_z[indx]\n",
    "    df.head()\n",
    "   \n",
    "\n",
    "    # fig = px.scatter_3d(df, x='X (mm)', y='Y (mm)', z='Z (mm)', color='Depth', title=\"ٌRadar Point Cloud\" , range_color=[min_depth,max_depth], range_y=[300,700],color_continuous_scale=color , opacity=1)\n",
    "    fig = px.scatter_3d(df, x='X (mm)', y='Y (mm)', z='Z (mm)', color='Depth', title=\"ٌRadar Point Cloud\" , range_color=[min_depth,max_depth],color_continuous_scale=color , opacity=1)\n",
    "    fig.update_traces(marker=dict(size=marker_size, line=dict(width=0))) \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_3D_threshold(\"ahmed_image_40cm_100\",1,'y',600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_dist = 0\n",
    "max_dist=800\n",
    "bin_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist(exp_name,folder_indx):\n",
    "    np.seterr(divide='ignore', invalid='ignore')\n",
    "    folder = [\"3D_Experements\", \"flat_Experements\"]\n",
    "    dist = np.loadtxt(folder[folder_indx]+\"/\"+exp_name+\"_x.txt\")\n",
    "    upper_angle = np.loadtxt(folder[folder_indx]+\"/\"+exp_name+\"_y.txt\")\n",
    "    lower_angle = np.loadtxt(folder[folder_indx]+\"/\"+exp_name+\"_z.txt\")\n",
    "\n",
    "    my_sample_y = np.array(dist)*np.cos(upper_angle)*np.cos(lower_angle)\n",
    "    \n",
    "    \n",
    "    my_sample_y = my_sample_y[~np.isnan(my_sample_y)]\n",
    "    \n",
    "    # plt.hist(my_sample_y , bins = [i for i in range(200,800,10)]) \n",
    "    # plt.title(\"histogram\") \n",
    "    # thresh_min = threshold_minimum(my_sample_y)\n",
    "    # print(thresh_min)\n",
    "    # plt.axvline(thresh_min, color='r')\n",
    "    # plt.show()  \n",
    "\n",
    "    #######################################\n",
    "    \n",
    "\n",
    "\n",
    "    # Set total number of bins in the histogram\n",
    "    bins_num = [i for i in range(min_dist,max_dist,bin_size)]#256\n",
    "\n",
    "    \n",
    "    # Get the image histogram\n",
    "    n = np.histogram(my_sample_y , bins = bins_num) \n",
    "    hist = n[0]\n",
    "    bin_edges = n[1]\n",
    "#     print (len(hist))\n",
    "#     print (len(bin_edges))\n",
    "    \n",
    "    # Get normalized histogram if it is required\n",
    "#     if is_normalized:\n",
    "#     hist = np.divide(hist.ravel(), hist.max())\n",
    "#     hist = preprocessing.normalize([hist])\n",
    "    # Calculate centers of bins\n",
    "    bin_mids = (bin_edges[:-1] + bin_edges[1:]) / 2.\n",
    "    \n",
    "    # print (\"bin_edges[:-1]\" , bin_edges[:-1])\n",
    "    # print (\"bin_edges[1:]\", bin_edges[1:])\n",
    "    # Iterate over all thresholds (indices) and get the probabilities w1(t), w2(t)\n",
    "    weight1 = np.cumsum(hist)\n",
    "    \n",
    "    weight2 = np.cumsum(hist[::-1])[::-1]\n",
    "    \n",
    "    # Get the class means mu0(t)\n",
    "    mean1 = np.cumsum(hist * bin_mids) / weight1\n",
    "    # Get the class means mu1(t)\n",
    "    mean2 = (np.cumsum((hist * bin_mids)[::-1]) / weight2[::-1])[::-1]\n",
    "\n",
    "    mean1 = np.nan_to_num(mean1)\n",
    "    mean2 = np.nan_to_num(mean2)\n",
    "    \n",
    "\n",
    "    \n",
    "    inter_class_variance = weight1[:-1] * weight2[1:] * (mean1[:-1] - mean2[1:]) ** 2\n",
    "\n",
    "    # Maximize the inter_class_variance function val\n",
    "    index_of_max_val = np.argmax(inter_class_variance)\n",
    "\n",
    "    threshold = bin_mids[:-1][index_of_max_val]\n",
    "#     print(\"Otsu's algorithm implementation thresholding result: \", threshold)\n",
    "\n",
    "    \n",
    "#     plt.title(\"histogram\") \n",
    "#     plt.axvline(threshold, color='r')\n",
    "#     plt.show()\n",
    "\n",
    "    hist = [ hist[i] if bin_edges[i] < threshold else 0 for i in range(hist.size)]\n",
    "    my_sample_y = my_sample_y[my_sample_y < threshold]\n",
    "    \n",
    "    total_value = np.sum(hist)\n",
    "    hist_prob = hist / total_value\n",
    "    hist_cumsum = np.cumsum(hist_prob)\n",
    "    new_hist = hist_cumsum * 800\n",
    "#     print (\"hist_cumsum:: \",hist_cumsum)\n",
    "    new_hist = np.floor(new_hist).astype(int)\n",
    "#     print (\"new_hist:: \",new_hist)\n",
    "    new_my_sample_y = np.zeros(len(my_sample_y))\n",
    "    for idx in range (len(my_sample_y)):\n",
    "        for i in range(len(bin_edges)):\n",
    "            if bin_edges[i]> my_sample_y[idx] :\n",
    "#                 print(\"my_sample_y\",my_sample_y[idx],\"bin_edges\",bin_edges[i],\"i\",i,\"new_hist\" ,new_hist[i-1])\n",
    "                new_my_sample_y[idx] = new_hist[i-1]\n",
    "                break\n",
    "#     print(\"len_ny\",new_my_sample_y.shape)\n",
    "#     print(\"len_oy\",my_sample_y.shape)\n",
    "    \n",
    "#     plt.title(\"histogram_2\")\n",
    "    \n",
    "    n = np.histogram(new_my_sample_y , bins = bins_num) \n",
    "    hist_new = n[0]\n",
    "    bin_edges_new = n[1]\n",
    "#     plt.show()\n",
    "#     x = my_sample_y\n",
    "#     y = new_my_sample_y\n",
    "\n",
    "#     bins = bins_num\n",
    "\n",
    "#     pyplot.hist(x, bins_num, alpha=0.5, label='x')|\n",
    "#     pyplot.hist(y, bins_num, alpha=0.5, label='y')\n",
    "#     pyplot.legend(loc='upper right')|\n",
    "#     pyplot.show()\n",
    "    \n",
    "    \n",
    "#     print(\"oh\",hist)\n",
    "#     print(\"nh\",hist_n)\n",
    "#     print(\"oy\",bin_edges)\n",
    "#     print(\"ny\",new_my_sample_y)\n",
    "    return threshold,hist,bin_edges,hist_new,bin_edges_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t , h, e ,h_n , e_n = plot_hist(\"ta7a_14_7_0\",0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "############################################## SVM ###################################################################\n",
    "# This function is used to create separated dataset files \n",
    "# for each experement \n",
    "def create_dataset():\n",
    "\n",
    "    exp_p = [\n",
    "#         \"waleed_60CM\", \n",
    "#             \"nassar_50cm\",\n",
    "#             \"gedo_50cm\",\n",
    "#              \"gedo_14_7\",\n",
    "#                 \"waleed_14_7\",\n",
    "#                 \"ta7a_14_7\",\n",
    "#                 \"nassar_14_7\"\n",
    "#         \"nassar_mask_glasses\"\n",
    "#         \"gedo_mask_glass\"\n",
    "#         \"gedo_bro2\",\n",
    "#         \"gedo_sis2\",\n",
    "#         \"gedo_sis1\",\n",
    "#         \"gedo_sis1_headphone\",\n",
    "#         \"gedo_mum\",\n",
    "#         \"gedo_bro1_mask_glass\",\n",
    "#         \"gedo_bro1\",\n",
    "#         \"waleed_noisy\",\n",
    "#         \"waleed_bad_ex\"\n",
    "#         \"ahmed_mum_40cm\",\n",
    "#         \"ahmed_mum2\"\n",
    "    ]\n",
    "\n",
    "    exp_n = [\n",
    "#         \"painting_50cm_rotation2\",\n",
    "#         \"painting_50cm_rotation\",\n",
    "#         \"painting_50cm\",\n",
    "#         \"painting_40cm_with_angle\",\n",
    "#         \"painting_40cm\",\n",
    "#     \"metal_dish\",\n",
    "#             \"painting\", \n",
    "#             \"ahmed_image_40cm\", \n",
    "#             \"ahmed_image_40cm_with_chair_behind\", \n",
    "#             \"ahmed_image_50cm\", \n",
    "#             \"ahmed_image_50cm_with_chair_behind\", \n",
    "#             \"ahmed_image_60cm\", \n",
    "#             \"ahmed_image_60cm_with_chair_behind\", \n",
    "#             \"Ball_40cm\", \n",
    "#             \"Ball_50cm\", \n",
    "#             \"Ball_60cm\", \n",
    "#             \"basket_40cm\", \n",
    "#             \"basket_50cm\", \n",
    "#             \"basket_60cm\", \n",
    "#             \"vas\"\n",
    "#         \"air\"\n",
    "#             \"noisy_air\"\n",
    "        \"wall_22_7\"\n",
    "            ]\n",
    "    count = 0\n",
    "\n",
    "    for i in range(len(exp_n)):\n",
    "        feature_hist = []\n",
    "        target_list = []\n",
    "        for j in range(1000):\n",
    "            exp_name = exp_n[i] + \"_\" + str(j)\n",
    "            t , h ,e , h_n, e_n= plot_hist(exp_name,1)\n",
    "            # h = h/np.sum(h)\n",
    "#             f = [ h[n] if e[n] < t else 0 for n in range(h.size)]\n",
    "            feature_hist.append(h_n)\n",
    "            target_list.append(-1)\n",
    "            count += 1\n",
    "            print(\"count :: \",count)\n",
    "        feature_hist = np.array(feature_hist)\n",
    "        target_list = np.array(target_list)\n",
    "        \n",
    "        file_data = open(\"equalized_feature/\"+exp_n[i]+\"_features.txt\", \"a\")\n",
    "        np.savetxt(file_data, feature_hist)\n",
    "        file_data.close()\n",
    "\n",
    "        file_target_list = open(\"equalized_feature/\"+exp_n[i]+\"_target_list.txt\", \"a\")\n",
    "        np.savetxt(file_target_list, target_list)\n",
    "        file_target_list.close()\n",
    "        \n",
    "    for i in range(len(exp_p)):\n",
    "        feature_hist = []\n",
    "        target_list = []\n",
    "        for j in range(1000):\n",
    "            exp_name = exp_p[i] + \"_\" + str(j)\n",
    "            t , h ,e ,h_n , e_n = plot_hist(exp_name,0)\n",
    "            # h = h/np.sum(h)\n",
    "#             f = [ h[n] if e[n] < t else 0 for n in range(h.size)]\n",
    "            feature_hist.append(h_n)\n",
    "            target_list.append(1)\n",
    "            count += 1\n",
    "            print(\"count :: \",count)\n",
    "        feature_hist = np.array(feature_hist)\n",
    "        target_list = np.array(target_list)\n",
    "        \n",
    "        file_data = open(\"equalized_feature/\"+exp_p[i]+\"_features.txt\", \"a\")\n",
    "        np.savetxt(file_data, feature_hist)\n",
    "        file_data.close()\n",
    "\n",
    "        file_target_list = open(\"equalized_feature/\"+exp_p[i]+\"_target_list.txt\", \"a\")\n",
    "        np.savetxt(file_target_list, target_list)\n",
    "        file_target_list.close()\n",
    "    return \"done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "############################################## SVM ###################################################################\n",
    "print(create_dataset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################## SVM ###################################################################\n",
    "# In this cell we will read data in one array to train \n",
    "def pre_processing(exp_name):\n",
    "    target_list = np.loadtxt(\"equalized_feature/\"+exp_name+\"_target_list.txt\")\n",
    "    data_list = np.loadtxt(\"equalized_feature/\"+exp_name+\"_features.txt\")\n",
    "    for j in range(1000):\n",
    "        data_list[j] = (data_list[j] - np.min(data_list[j])) / (np.max(data_list[j]) - np.min(data_list[j]))\n",
    "#         temp = data_list[j]\n",
    "#         temp = temp[temp != 0]\n",
    "#         temp = np.trim_zeros(temp)\n",
    "#         N = 59 - len(temp)\n",
    "#         temp = np.pad(temp, (0, N), 'constant')\n",
    "#         data_list[j] = temp\n",
    "    \n",
    "    print(np.sum(data_list))\n",
    "    return data_list,target_list\n",
    "\n",
    "exp_p = [\n",
    "        \"waleed_60CM\", \n",
    "        \"nassar_50cm\",\n",
    "        \"gedo_50cm\",\n",
    "        \"gedo_14_7\",\n",
    "        \"waleed_14_7\",\n",
    "        \"ta7a_14_7\",\n",
    "        \"nassar_14_7\",\n",
    "        \"waleed_mask_glass\",\n",
    "        \"nassar_mask_glasses\",\n",
    "        \"gedo_mask_glass\",\n",
    "        \"gedo_bro2\",\n",
    "        \"gedo_sis2\",\n",
    "        \"gedo_sis1\",\n",
    "        \"gedo_sis1_headphone\",\n",
    "        \"gedo_mum\",\n",
    "        \"gedo_bro1_mask_glass\",\n",
    "        \"gedo_bro1\",\n",
    "        \"waleed_noisy\",\n",
    "        \"waleed_bad_ex\",\n",
    "         \"ahmed_mum_40cm\",\n",
    "        \"ahmed_mum2\"\n",
    "        ]\n",
    "\n",
    "exp_n = [\"metal_dish\",\n",
    "        \"painting\", \n",
    "        \"ahmed_image_40cm\", \n",
    "        \"ahmed_image_40cm_with_chair_behind\", \n",
    "        \"ahmed_image_50cm\", \n",
    "        \"ahmed_image_50cm_with_chair_behind\", \n",
    "        \"ahmed_image_60cm\", \n",
    "        \"ahmed_image_60cm_with_chair_behind\",\n",
    "        \"air\",\n",
    "        \"painting_50cm_rotation2\",\n",
    "        \"painting_50cm_rotation\",\n",
    "        \"painting_50cm\",\n",
    "        \"painting_40cm_with_angle\",\n",
    "        \"painting_40cm\",\n",
    "         \"noisy_air\",\n",
    "         \"wall_22_7\"\n",
    "\n",
    "#          \"Ball_40cm\", \n",
    "#         \"Ball_50cm\", \n",
    "#         \"Ball_60cm\", \n",
    "#         \"basket_40cm\",\n",
    "#         \"basket_50cm\", \n",
    "#         \"basket_60cm\"\n",
    "#          , \"vas\"\n",
    "        ]\n",
    "         \n",
    "total_data = []\n",
    "total_target_lists = []\n",
    "for i in range(len(exp_n)):     \n",
    "    data_list , target_list = pre_processing(exp_n[i])  \n",
    "    target_list = target_list * 0 \n",
    "    target_list = target_list.tolist()\n",
    "    data_list = data_list.tolist()\n",
    "    total_data.extend(data_list)\n",
    "    total_target_lists.extend(target_list)\n",
    "    \n",
    "for i in range(len(exp_p)):\n",
    "    data_list , target_list = pre_processing(exp_p[i])   \n",
    "    target_list = target_list.tolist()\n",
    "    data_list = data_list.tolist()\n",
    "    total_data.extend(data_list)\n",
    "    total_target_lists.extend(target_list)\n",
    "    \n",
    "total_data = np.array(total_data)\n",
    "total_target_lists = np.array(total_target_lists)\n",
    "total_target_lists = np.reshape(total_target_lists , (-1,1))\n",
    "# print(total_data.shape)\n",
    "total_data = np.concatenate((total_data, total_target_lists), axis=1)\n",
    "print(total_data[0])\n",
    "np.random.shuffle(total_data)\n",
    "\n",
    "print(total_data[0])\n",
    "\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################## SVM ###################################################################\n",
    "# Import train_test_split function\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "\n",
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(total_data[:,0:199], total_data[:,199], train_size=0.65,test_size=0.35, random_state=101) # 70% training and 30% test\n",
    "# print (X_train)\n",
    "#Create a svm Classifier\n",
    "\n",
    "\n",
    "\n",
    "clf = svm.SVC(kernel='rbf' ) # Linear Kernel\n",
    "\n",
    "#Train the model using the training sets\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "# print (X_test.shape)\n",
    "y_pred = clf.predict(X_test)\n",
    "# Model Accuracy: how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################## SVM ###################################################################\n",
    "t , h, e , h_n , e_n = plot_hist(\"test_wall_22_7\",1)\n",
    "# print(t,e,h.size)\n",
    "# f = [ h[n] if e[n] < t else 0 for n in range(h.size)]\n",
    "# print(f)\n",
    "# f =  100 *(f / np.sum(f))\n",
    "# print(f)\n",
    "f = (h_n - np.min(h_n)) /(np.max(h_n) - np.min(h_n))\n",
    "# f = np.trim_zeros(f)\n",
    "# # print(f)\n",
    "# N = 59 - len(f)\n",
    "# f = np.pad(f, (0, N), 'constant')\n",
    "# print(f)\n",
    "data_list = np.array([f])\n",
    "y_pred = clf.predict(data_list)\n",
    "# y_pred[1600]\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################## SVM ###################################################################\n",
    "result_p = []\n",
    "result_n = []\n",
    "count = 0\n",
    "for i in range(1000):\n",
    "    t , h, e , h_n , e_n = plot_hist(\"wall_22_7_\"+str(i),1)\n",
    "    # print(t,e,h.size)\n",
    "    # f = [ h[n] if e[n] < t else 0 for n in range(h.size)]\n",
    "    # print(f)\n",
    "\n",
    "    # f =  100 *(f / np.sum(f))\n",
    "    # print(f)\n",
    "    f = (h_n - np.min(h_n)) /(np.max(h_n) - np.min(h_n))\n",
    "    # f = np.trim_zeros(f)\n",
    "    # # print(f)\n",
    "    \n",
    "    # N = 59 - len(f)\n",
    "    # f = np.pad(f, (0, N), 'constant')\n",
    "    # print(f)\n",
    "    data_list = np.array([f])\n",
    "    y_pred = clf.predict(data_list)\n",
    "#     y_pred =nn_pred(data_list)\n",
    "    count +=1\n",
    "    print(\"count :: \",count)\n",
    "    if y_pred == True or y_pred == 1:\n",
    "        result_p.append(y_pred)\n",
    "    else:\n",
    "        result_n.append(y_pred)\n",
    "        \n",
    "        \n",
    "print(\"number of positive results :: \", len(result_p))\n",
    "print(\"number of negative results :: \", len(result_n))\n",
    "print(\"positive accuracy :: \", (100 *len(result_p)/(len(result_p)+len(result_n)) ))\n",
    "print(\"negative accuracy :: \", (100 *len(result_n)/(len(result_p)+len(result_n)) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def points_to_image(exp_name,folder_indx,threshold):\n",
    "    np.seterr(divide='ignore', invalid='ignore')\n",
    "    folder = [\"3D_Experements\", \"flat_Experements\"]\n",
    "    dist = np.loadtxt(folder[folder_indx]+\"/\"+exp_name+\"_x.txt\")\n",
    "    upper_angle = np.loadtxt(folder[folder_indx]+\"/\"+exp_name+\"_y.txt\")\n",
    "    lower_angle = np.loadtxt(folder[folder_indx]+\"/\"+exp_name+\"_z.txt\")\n",
    "\n",
    "#     print(np.amax(upper_angle*180/np.pi))\n",
    "#     print(np.amin(upper_angle*180/np.pi))\n",
    "#     print(np.amax(lower_angle*180/np.pi))\n",
    "#     print(np.amin(lower_angle*180/np.pi))\n",
    "    \n",
    "\n",
    "    #x , y , z = np.array(dist)*np.cos(uAngel)*np.sin(lAngel) , np.array(dist)*np.cos(uAngel)*np.cos(lAngel) , np.array(dist)*np.sin(uAngel)\n",
    "\n",
    "    my_sample_x = np.array(dist)*np.cos(upper_angle)*np.sin(lower_angle)\n",
    "    my_sample_y =  np.array(dist)*np.cos(upper_angle)*np.cos(lower_angle)\n",
    "    my_sample_z =  np.array(dist)*np.sin(upper_angle)\n",
    "    \n",
    "    my_sample_x = my_sample_x[~np.isnan(my_sample_x)]\n",
    "    my_sample_y = my_sample_y[~np.isnan(my_sample_y)]\n",
    "    my_sample_z = my_sample_z[~np.isnan(my_sample_z)]\n",
    "\n",
    "    \n",
    "    min_depth = np.amin(my_sample_y)\n",
    "    max_depth = (min_depth + threshold)\n",
    "    indx = my_sample_y <= (min_depth + threshold)\n",
    "\n",
    "    my_sample_x = my_sample_x[indx]\n",
    "    my_sample_y = my_sample_y[indx]\n",
    "    my_sample_z = my_sample_z[indx]\n",
    "    my_sample_y = 1-  (my_sample_y -  np.min(my_sample_y)) / (np.max(my_sample_y) - np.min(my_sample_y))\n",
    "    x_min = np.min(my_sample_x)\n",
    "    x_max = np.max(my_sample_x)\n",
    "    y_min = np.min(my_sample_y)\n",
    "    y_max = np.max(my_sample_y)\n",
    "    z_min = np.min(my_sample_z)\n",
    "    z_max = np.max(my_sample_z)\n",
    "    width = 1+(x_max - x_min).astype(int)\n",
    "    hight = 1+(z_max - z_min).astype(int)\n",
    "\n",
    "    # my_sample_y = my_sample_y + (1-y_max)\n",
    "#     print(\"img :: \",width,hight)\n",
    "    img = np.zeros((width,hight))\n",
    "    \n",
    "    for i in range(len(my_sample_x)):\n",
    "        x = (my_sample_x[i] - x_min).astype(int)\n",
    "        z = (my_sample_z[i] - z_min).astype(int)\n",
    "#         print(\"x,z :: \",x,z)\n",
    "        img[x,z] = my_sample_y[i]\n",
    "#     print('Resized Dimensions : ',resized.shape)\n",
    "\n",
    "#     cv2.imshow(\"Resized image\", resized)\n",
    "#     cv2.waitKey(0)\n",
    "#     cv2.destroyAllWindows()\n",
    "#     img = Image.fromarray(img)\n",
    "#     print(len(my_sample_x))\n",
    "    \n",
    "    kernel = np.ones((5,5), np.uint8)\n",
    "#     # The first parameter is the original image,\n",
    "#     # kernel is the matrix with which image is\n",
    "#     # convolved and third parameter is the number\n",
    "#     # of iterations, which will determine how much\n",
    "#     # you want to erode/dilate a given image.\n",
    "#     img_erosion = cv2.erode(img, kernel, iterations=1)\n",
    "    img_dilation = cv2.dilate(img, kernel, iterations=3)\n",
    "\n",
    "#     cv2.imshow('Input', img)\n",
    "#     cv2.imshow('Erosion', img_erosion)\n",
    "#     cv2.imshow('Dilation', img_dilation)\n",
    "\n",
    "#     cv2.waitKey(0)\n",
    "#     img = Image.fromarray(resized)\n",
    "#     img.show()\n",
    "\n",
    "    dim = (64, 64)    \n",
    "    resized = cv2.resize(img_dilation, dim, interpolation = cv2.INTER_AREA)\n",
    "    \n",
    "    return resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################# CNN #################################\n",
    "# CNN Variables \n",
    "model_name = 'cnn/small_trained_data.h5'\n",
    "batch_size = 64\n",
    "epochs = 20\n",
    "num_classes = 2\n",
    "dim = 64\n",
    "regularizer = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################# CNN #################################\n",
    "def create_2dimg_dataset():\n",
    "\n",
    "    exp_p = [\n",
    "        \"waleed_60CM\", \n",
    "        \"nassar_50cm\",\n",
    "        \"gedo_50cm\",\n",
    "        \"gedo_14_7\",\n",
    "        \"waleed_14_7\",\n",
    "        \"ta7a_14_7\",\n",
    "        \"nassar_14_7\",\n",
    "        \"nassar_mask_glasses\",\n",
    "        \"gedo_mask_glass\",\n",
    "        \"gedo_bro2\",\n",
    "        \"gedo_sis2\",\n",
    "        \"gedo_sis1\",\n",
    "        \"gedo_sis1_headphone\",\n",
    "        \"gedo_mum\",\n",
    "        \"gedo_bro1_mask_glass\",\n",
    "        \"gedo_bro1\",\n",
    "        \"waleed_noisy\",\n",
    "        \"waleed_bad_ex\",\n",
    "        \"ahmed_mum_40cm\",\n",
    "        \"ahmed_mum2\"\n",
    "    ]\n",
    "\n",
    "    exp_n = [\n",
    "        \"painting_50cm_rotation2\",\n",
    "        \"painting_50cm_rotation\",\n",
    "        \"painting_50cm\",\n",
    "        \"painting_40cm_with_angle\",\n",
    "        \"painting_40cm\",\n",
    "        \"metal_dish\",\n",
    "        \"painting\", \n",
    "        \"ahmed_image_40cm\", \n",
    "        \"ahmed_image_40cm_with_chair_behind\", \n",
    "        \"ahmed_image_50cm\", \n",
    "        \"ahmed_image_50cm_with_chair_behind\", \n",
    "        \"ahmed_image_60cm\", \n",
    "        \"ahmed_image_60cm_with_chair_behind\", \n",
    "        \"Ball_40cm\", \n",
    "        \"Ball_50cm\", \n",
    "        \"Ball_60cm\", \n",
    "        \"basket_40cm\", \n",
    "        \"basket_50cm\", \n",
    "        \"basket_60cm\", \n",
    "        \"vas\",\n",
    "        \"air\",\n",
    "        \"noisy_air\",\n",
    "        \"wall_22_7\"\n",
    "            ]\n",
    "    count = 0\n",
    "\n",
    "    for i in range(len(exp_n)):\n",
    "        feature_img = []\n",
    "        target_list = []\n",
    "        for j in range(1000):\n",
    "            exp_name = exp_n[i] + \"_\" + str(j)\n",
    "            t , h ,e , h_n, e_n= plot_hist(exp_name,1)\n",
    "            img = points_to_image(exp_name,1,t)\n",
    "            feature_img.append(img)\n",
    "            target_list.append(0)\n",
    "            count += 1\n",
    "            print(\"count :: \",count)\n",
    "        feature_img = np.array(feature_img)\n",
    "        target_list = np.array(target_list)\n",
    "        np.save(\"img_feature/64x64/\"+exp_n[i]+\"_features.npy\",feature_img)\n",
    "        np.save(\"img_feature/64x64/\"+exp_n[i]+\"_target_list.npy\",target_list)\n",
    "\n",
    "        \n",
    "    for i in range(len(exp_p)):\n",
    "        feature_img = []\n",
    "        target_list = []\n",
    "        for j in range(1000):\n",
    "            exp_name = exp_p[i] + \"_\" + str(j)\n",
    "            t , h ,e , h_n, e_n= plot_hist(exp_name,0)\n",
    "            img = points_to_image(exp_name,0,t)\n",
    "            feature_img.append(img)\n",
    "            target_list.append(1)\n",
    "            count += 1\n",
    "            print(\"count :: \",count)\n",
    "        feature_img = np.array(feature_img)\n",
    "        target_list = np.array(target_list)\n",
    "        np.save(\"img_feature/64x64/\"+exp_p[i]+\"_features.npy\",feature_img)\n",
    "        np.save(\"img_feature/64x64/\"+exp_p[i]+\"_target_list.npy\",target_list)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_2dimg_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################# CNN #################################\n",
    "# Initialising the CNN\n",
    "model = Sequential()\n",
    "\n",
    "# Step 1 - Convolution\n",
    "model.add(Conv2D(32, (3, 3), input_shape = (dim,dim,1), activation = 'relu' , kernel_regularizer=regularizers.l1(regularizer)))\n",
    "\n",
    "# Step 2 - Pooling\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Adding a second convolutional layer\n",
    "model.add(Conv2D(64, (3, 3), activation = 'relu' , kernel_regularizer=regularizers.l1(regularizer)))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Adding a third convolutional layer\n",
    "model.add(Conv2D(128, (3, 3), activation = 'relu', kernel_regularizer=regularizers.l1(regularizer)))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Adding a fourth convolutional layer\n",
    "model.add(Conv2D(128, (3, 3), activation = 'relu', kernel_regularizer=regularizers.l1(regularizer)))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Step 3 - Flattening\n",
    "model.add(Flatten())\n",
    "\n",
    "# Step 4 - Full connection\n",
    "model.add(Dense(units = 64, activation = 'relu' , kernel_regularizer=regularizers.l1(regularizer)))\n",
    "model.add(Dense(units = 1, activation = 'sigmoid' , kernel_regularizer=regularizers.l1(regularizer)))\n",
    "\n",
    "# Compiling the CNN\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################# CNN #################################\n",
    "model.summary()\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################# CNN #################################\n",
    "datagen = ImageDataGenerator(\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')\n",
    "\n",
    "def unison_shuffled_copies(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]\n",
    "\n",
    "def apply_transoformation(imgs):\n",
    "    num_generated = 4\n",
    "    final_result =[]\n",
    "    for img in imgs:\n",
    "        img = img.reshape((1,1,) + img.shape)  # this is a Numpy array with shape (1, 3, 150, 150)\n",
    "        final_result.append(img)\n",
    "        i = 0\n",
    "        for batch in datagen.flow(img, batch_size=1):\n",
    "            i += 1\n",
    "            final_result.append(batch)\n",
    "            \n",
    "            if i == num_generated:\n",
    "                break  # otherwise the ge\n",
    "                \n",
    "    final_result = np.array(final_result)\n",
    "    final_result = final_result.reshape(-1,dim,dim)\n",
    "    return final_result\n",
    "\n",
    "def generate_test_set():\n",
    "    exp_p = [\n",
    "#             \"waleed_60CM\", \n",
    "#             \"nassar_50cm\",\n",
    "#             \"gedo_50cm\",\n",
    "#             \"gedo_14_7\",\n",
    "#             \"waleed_14_7\",\n",
    "#             \"ta7a_14_7\",\n",
    "#             \"nassar_14_7\",\n",
    "#             \"nassar_mask_glasses\",\n",
    "#             \"gedo_mask_glass\",\n",
    "#             \"gedo_bro2\",\n",
    "#             \"gedo_sis2\",\n",
    "#             \"gedo_sis1\",\n",
    "#             \"gedo_sis1_headphone\",\n",
    "            \"gedo_mum\",\n",
    "            \"gedo_bro1_mask_glass\",\n",
    "#             \"gedo_bro1\",\n",
    "#             \"waleed_noisy\",\n",
    "#             \"waleed_bad_ex\",\n",
    "#             \"ahmed_mum_40cm\",\n",
    "#             \"ahmed_mum2\"\n",
    "    ]\n",
    "\n",
    "    exp_n = [\n",
    "#             \"painting_50cm_rotation2\",\n",
    "#             \"painting_50cm_rotation\",\n",
    "#             \"painting_50cm\",\n",
    "#             \"painting_40cm_with_angle\",\n",
    "#             \"painting_40cm\",\n",
    "#             \"metal_dish\",\n",
    "#             \"painting\", \n",
    "#             \"ahmed_image_40cm\", \n",
    "#             \"ahmed_image_40cm_with_chair_behind\", \n",
    "#             \"ahmed_image_50cm\", \n",
    "#             \"ahmed_image_50cm_with_chair_behind\", \n",
    "#             \"ahmed_image_60cm\", \n",
    "#             \"ahmed_image_60cm_with_chair_behind\", \n",
    "    #         \"Ball_40cm\", \n",
    "    #         \"Ball_50cm\", \n",
    "    #         \"Ball_60cm\", \n",
    "    #         \"basket_40cm\", \n",
    "    #         \"basket_50cm\", \n",
    "    #         \"basket_60cm\", \n",
    "    #         \"vas\",\n",
    "            \"air\",\n",
    "#             \"noisy_air\",\n",
    "            \"wall_22_7\"\n",
    "            ]\n",
    "\n",
    "\n",
    "    count = 0\n",
    "    size = max(len(exp_n),len(exp_p))\n",
    "    total_data = []\n",
    "    total_target_lists = []\n",
    "    print (\"Start Loading validation Data.\")\n",
    "    for i in range(size):\n",
    "\n",
    "        if i < len(exp_n):\n",
    "            data_list , target_list = np.load(\"img_feature/64x64/\"+exp_n[i]+\"_features.npy\"),np.load(\"img_feature/64x64/\"+exp_n[i]+\"_target_list.npy\")  \n",
    "            data_list = apply_transoformation(data_list)\n",
    "            target_list = np.repeat(target_list, len(data_list)/ 1000)\n",
    "            target_list = target_list \n",
    "            target_list = target_list.tolist()\n",
    "            data_list = data_list.tolist()\n",
    "            total_data.extend(data_list)\n",
    "            total_target_lists.extend(target_list)\n",
    "\n",
    "        if i < len(exp_p):\n",
    "            data_list , target_list = np.load(\"img_feature/64x64/\"+exp_p[i]+\"_features.npy\"),np.load(\"img_feature/64x64/\"+exp_p[i]+\"_target_list.npy\")  \n",
    "            data_list = apply_transoformation(data_list)\n",
    "            target_list = np.repeat(target_list, len(data_list)/ 1000)\n",
    "            target_list = target_list.tolist()\n",
    "            data_list = data_list.tolist()\n",
    "            total_data.extend(data_list)\n",
    "            total_target_lists.extend(target_list)\n",
    "        count+= 1 \n",
    "        print(\"Count :: \", count , \", Goal :: \",size)\n",
    "    total_data = np.array(total_data)\n",
    "    total_target_lists = np.array(total_target_lists)\n",
    "    total_data = np.reshape(total_data,(-1, dim,dim, 1))\n",
    "    return total_data,total_target_lists\n",
    "\n",
    "def train_CNN_model ():\n",
    "    exp_p = [\n",
    "            \"waleed_60CM\", \n",
    "            \"nassar_50cm\",\n",
    "#             \"gedo_50cm\",\n",
    "#             \"gedo_14_7\",\n",
    "#             \"waleed_14_7\",\n",
    "            \"ta7a_14_7\",\n",
    "#             \"nassar_14_7\",\n",
    "            \"nassar_mask_glasses\",\n",
    "#             \"gedo_mask_glass\",\n",
    "#             \"gedo_bro2\",\n",
    "            \"gedo_sis2\",\n",
    "#             \"gedo_sis1\",\n",
    "#             \"gedo_sis1_headphone\",\n",
    "#             \"gedo_mum\",\n",
    "#             \"gedo_bro1_mask_glass\",\n",
    "            \"gedo_bro1\",\n",
    "            \"waleed_noisy\",\n",
    "#             \"waleed_bad_ex\",\n",
    "#             \"ahmed_mum_40cm\",\n",
    "            \"ahmed_mum2\"\n",
    "    ]\n",
    "\n",
    "    exp_n = [\n",
    "#             \"painting_50cm_rotation2\",\n",
    "            \"painting_50cm_rotation\",\n",
    "            \"painting_50cm\",\n",
    "            \"painting_40cm_with_angle\",\n",
    "#             \"painting_40cm\",\n",
    "            \"metal_dish\",\n",
    "#             \"painting\", \n",
    "            \"ahmed_image_40cm\", \n",
    "#             \"ahmed_image_40cm_with_chair_behind\", \n",
    "#             \"ahmed_image_50cm\", \n",
    "            \"ahmed_image_50cm_with_chair_behind\", \n",
    "#             \"ahmed_image_60cm\", \n",
    "            \"ahmed_image_60cm_with_chair_behind\", \n",
    "    #         \"Ball_40cm\", \n",
    "    #         \"Ball_50cm\", \n",
    "    #         \"Ball_60cm\", \n",
    "    #         \"basket_40cm\", \n",
    "    #         \"basket_50cm\", \n",
    "    #         \"basket_60cm\", \n",
    "    #         \"vas\",\n",
    "#             \"air\",\n",
    "            \"noisy_air\",\n",
    "#             \"wall_22_7\"\n",
    "            ]\n",
    "\n",
    "\n",
    "    count = 0\n",
    "    size = max(len(exp_n),len(exp_p))\n",
    "    total_data = []\n",
    "    total_target_lists = []\n",
    "    print (\"Start Loading Training Data.\")\n",
    "    for i in range(size):\n",
    "\n",
    "        if i < len(exp_n):\n",
    "            data_list , target_list = np.load(\"img_feature/64x64/\"+exp_n[i]+\"_features.npy\"),np.load(\"img_feature/64x64/\"+exp_n[i]+\"_target_list.npy\")  \n",
    "            data_list = apply_transoformation(data_list)\n",
    "            target_list = np.repeat(target_list, len(data_list)/ 1000)\n",
    "            target_list = target_list \n",
    "            target_list = target_list.tolist()\n",
    "            data_list = data_list.tolist()\n",
    "            total_data.extend(data_list)\n",
    "            total_target_lists.extend(target_list)\n",
    "\n",
    "        if i < len(exp_p):\n",
    "            data_list , target_list = np.load(\"img_feature/64x64/\"+exp_p[i]+\"_features.npy\"),np.load(\"img_feature/64x64/\"+exp_p[i]+\"_target_list.npy\")  \n",
    "            data_list = apply_transoformation(data_list)\n",
    "            target_list = np.repeat(target_list, len(data_list)/ 1000)\n",
    "            target_list = target_list.tolist()\n",
    "            data_list = data_list.tolist()\n",
    "            total_data.extend(data_list)\n",
    "            total_target_lists.extend(target_list)\n",
    "        count+= 1 \n",
    "        print(\"Count :: \", count , \", Goal :: \",size)\n",
    "    total_data = np.array(total_data)\n",
    "    total_target_lists = np.array(total_target_lists)\n",
    "    total_data = np.reshape(total_data,(-1, dim,dim, 1))\n",
    "    #     total_target_lists = np.reshape(total_target_lists,(-1, 300,300, 1))\n",
    "    #     total_target_lists = np.reshape(total_target_lists , (-1,1))\n",
    "    # print(total_data.shape)\n",
    "    total_data , total_target_lists = unison_shuffled_copies(total_data , total_target_lists)\n",
    "#     print(total_data.shape)\n",
    "\n",
    "\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(total_data, total_target_lists, train_size=0.8,test_size=0.2, random_state=101) # 70% training and 30% test\n",
    "    X_train , y_train = total_data,total_target_lists\n",
    "    X_test , y_test = generate_test_set()\n",
    "    model = load_model(model_name)\n",
    "    print (\"Start Training.\")\n",
    "    fashion_train = model.fit(X_train, y_train, batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(X_test, y_test))\n",
    "    model.save(model_name)\n",
    "\n",
    "\n",
    "    print(\"Training is Done.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_CNN_model ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################# CNN #################################\n",
    "#             \"waleed_60CM\", \n",
    "#             \"gedo_50cm\",\n",
    "#             \"gedo_14_7\",\n",
    "#             \"waleed_14_7\",\n",
    "#             \"nassar_14_7\",\n",
    "#             \"nassar_mask_glasses\",\n",
    "#             \"gedo_mask_glass\",\n",
    "#             \"gedo_bro2\",\n",
    "#             \"gedo_sis1\",\n",
    "#             \"gedo_sis1_headphone\",\n",
    "#             \"gedo_mum\",\n",
    "#             \"gedo_bro1\",\n",
    "#             \"waleed_bad_ex\",\n",
    "#             \"ahmed_mum_40cm\",\n",
    "#             \"painting_50cm_rotation2\",\n",
    "#             \"painting_40cm_with_angle\",\n",
    "#             \"painting_40cm\",\n",
    "#             \"ahmed_image_40cm_with_chair_behind\", \n",
    "#             \"ahmed_image_50cm\", \n",
    "#             \"ahmed_image_50cm_with_chair_behind\", \n",
    "#             \"ahmed_image_60cm_with_chair_behind\", \n",
    "#             \"noisy_air\",\n",
    "\n",
    "# \"gedo_bro1_mask_glass\"\n",
    "exp_name = \"ahmed_image_50cm_with_chair_behind\"\n",
    "folder_indx = 0\n",
    "\n",
    "def nn_pred(img):\n",
    "    img = np.reshape(img,(1,64,64, 1))\n",
    "    result = model.predict(img)\n",
    "    print(\"result :: \",result[0][0])\n",
    "    return result[0][0] > .5\n",
    "\n",
    "def pred_one(exp_name,folder_indx):\n",
    "    t , h ,e , h_n, e_n= plot_hist(exp_name,folder_indx)\n",
    "    img = points_to_image(exp_name,folder_indx,t)\n",
    "    print(\"is this 3D object :: \" ,nn_pred(img))\n",
    "    \n",
    "exp_name = \"test_wall_23_7_220\"\n",
    "folder_indx = 1\n",
    "\n",
    "def pred_many(exp_name,folder_indx):\n",
    "    result_p = []\n",
    "    result_n = []\n",
    "    count = 0\n",
    "    for i in range(1000):\n",
    "        t , h ,e , h_n, e_n= plot_hist(exp_name,folder_indx)\n",
    "        img = points_to_image(exp_name,folder_indx,t)\n",
    "        count +=1\n",
    "        print(\"count :: \",count)\n",
    "        y_pred = nn_pred(img)\n",
    "        if y_pred == True or y_pred == 1:\n",
    "            result_p.append(y_pred)\n",
    "        else:\n",
    "            result_n.append(y_pred)\n",
    "\n",
    "\n",
    "    print(\"number of positive results :: \", len(result_p))\n",
    "    print(\"number of negative results :: \", len(result_n))\n",
    "    print(\"positive accuracy :: \", (100 *len(result_p)/(len(result_p)+len(result_n)) ))\n",
    "    print(\"negative accuracy :: \", (100 *len(result_n)/(len(result_p)+len(result_n)) ))\n",
    "    \n",
    "# pred_one(exp_name,folder_indx)\n",
    "pred_many(exp_name,folder_indx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################## NN ################################################################\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers,Sequential,models\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(total_data[:,0:199], total_data[:,199], train_size=0.7,test_size=0.3, random_state=101) # 70% training and 30% test\n",
    "model = keras.Sequential()\n",
    "model.add(layers.Dense(32,input_dim=199,activation='relu'))\n",
    "model.add(layers.Dense(16,activation='relu'))\n",
    "model.add(layers.Dense(8,activation='relu'))\n",
    "model.add(layers.Dense(4, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=10, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################## NN ################################################################\n",
    "def nn_pred(hist):\n",
    "    result = model.predict(np.reshape(hist,(-1,199)))\n",
    "    return result[0][0] > .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################## NN ################################################################\n",
    "t , h, e , h_n , e_n = plot_hist(\"test_waleed2_17_7\",0)\n",
    "f = (h_n - np.min(h_n)) /(np.max(h_n) - np.min(h_n))\n",
    "data_list = np.array([f])\n",
    "res = nn_pred(data_list)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate 1000 Sample from 1 Sample By Adding Randome Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "exp_name = \"test_waleed2_17_7\"\n",
    "\n",
    "dist = np.loadtxt(\"3D_Experements\"+\"/\"+exp_name+\"_x.txt\")\n",
    "upper_angle = np.loadtxt(\"3D_Experements\"+\"/\"+exp_name+\"_y.txt\")\n",
    "lower_angle = np.loadtxt(\"3D_Experements\"+\"/\"+exp_name+\"_z.txt\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#x , y , z = np.array(dist)*np.cos(uAngel)*np.sin(lAngel) , np.array(dist)*np.cos(uAngel)*np.cos(lAngel) , np.array(dist)*np.sin(uAngel)\n",
    "for i in range(1000):\n",
    "#     n = random.randint(5,20)\n",
    "    new_dist = dist\n",
    "#     for j in range(n):\n",
    "#         k = random.randint(0,5)\n",
    "#         l = random.randint(0,len(dist) - 1)\n",
    "#         new_dist[l] = new_dist[l] + k\n",
    "        \n",
    "    file_data = open(\"3D_Experements/waleed_bad_ex_\"+str(i)+\"_x.txt\", \"a\")\n",
    "    np.savetxt(file_data, new_dist)\n",
    "    file_data.close()\n",
    "    \n",
    "    file_data = open(\"3D_Experements/waleed_bad_ex_\"+str(i)+\"_y.txt\", \"a\")\n",
    "    np.savetxt(file_data, upper_angle)\n",
    "    file_data.close()\n",
    "    \n",
    "    file_data = open(\"3D_Experements/waleed_bad_ex_\"+str(i)+\"_z.txt\", \"a\")\n",
    "    np.savetxt(file_data, lower_angle)\n",
    "    file_data.close()\n",
    "    \n",
    "    print(i)\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unwanted Cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.load(\"img_feature/negative/wall_22_7_features.npy\")\n",
    "print(b.shape)\n",
    "f = b[0]\n",
    "\n",
    "print(f.shape)\n",
    "# img = Image.fromarray(f)\n",
    "# img.show()\n",
    "\n",
    "b = np.reshape(b,(-1, 300,300, 1))\n",
    "print(b.shape)\n",
    "f2 =b[0,:,:,:]\n",
    "print(f2.shape)\n",
    "cv2.imshow(\"Resized image\", f2)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3D (exp_name , depth_axis):\n",
    "    dist = np.loadtxt(\"3D_Experements/\"+exp_name+\"_x.txt\")\n",
    "    upper_angle = np.loadtxt(\"3D_Experements/\"+exp_name+\"_y.txt\")\n",
    "    lower_angle = np.loadtxt(\"3D_Experements/\"+exp_name+\"_z.txt\")\n",
    "\n",
    "    print(np.amax(upper_angle*180/np.pi))\n",
    "    print(np.amin(upper_angle*180/np.pi))\n",
    "    print(np.amax(lower_angle*180/np.pi))\n",
    "    print(np.amin(lower_angle*180/np.pi))\n",
    "    \n",
    "\n",
    "    #x , y , z = np.array(dist)*np.cos(uAngel)*np.sin(lAngel) , np.array(dist)*np.cos(uAngel)*np.cos(lAngel) , np.array(dist)*np.sin(uAngel)\n",
    "\n",
    "    my_sample_x = np.array(dist)*np.cos(upper_angle)*np.sin(lower_angle)\n",
    "    my_sample_y =  np.array(dist)*np.cos(upper_angle)*np.cos(lower_angle)\n",
    "    my_sample_z =  np.array(dist)*np.sin(upper_angle)\n",
    "    \n",
    "    my_sample_x = my_sample_x[~np.isnan(my_sample_x)]\n",
    "    my_sample_y = my_sample_y[~np.isnan(my_sample_y)]\n",
    "    my_sample_z = my_sample_z[~np.isnan(my_sample_z)]\n",
    "\n",
    "    df = pd.DataFrame(my_sample_x,columns=['X (mm)'])\n",
    "    df['Y (mm)'] = my_sample_y\n",
    "    df['Z (mm)'] = my_sample_z\n",
    "    df.head()\n",
    "    min_depth = 0\n",
    "    max_depth = 0\n",
    "    if depth_axis == 'x':\n",
    "        df['Depth'] = my_sample_x\n",
    "        max_depth = np.amax(my_sample_x)\n",
    "        min_depth = np.amin(my_sample_x)\n",
    "\n",
    "    elif depth_axis == 'y':\n",
    "        df['Depth'] = my_sample_y\n",
    "        max_depth = np.amax(my_sample_y)\n",
    "        min_depth = np.amin(my_sample_y)\n",
    "\n",
    "    elif depth_axis == 'z':\n",
    "        df['Depth'] = my_sample_z\n",
    "        max_depth = np.amax(my_sample_z)\n",
    "        min_depth = np.amin(my_sample_z)\n",
    "        \n",
    "\n",
    "    else:\n",
    "        print(\"invalid depth axis\")\n",
    "        return\n",
    "   \n",
    "\n",
    "    fig = px.scatter_3d(df, x='X (mm)', y='Y (mm)', z='Z (mm)', color='Depth', title=\"ٌRadar Point Cloud\" , range_color=[min_depth-200,max_depth+200],color_continuous_scale=color , opacity=1)\n",
    "    fig.update_traces(marker=dict(size=marker_size, line=dict(width=0))) \n",
    "    fig.show()\n",
    "\n",
    "\n",
    "def plot_2D (exp_name):\n",
    "    dist = np.loadtxt(\"3D_Experements/\"+exp_name+\"_x.txt\")\n",
    "    upper_angle = np.loadtxt(\"3D_Experements/\"+exp_name+\"_y.txt\")\n",
    "    lower_angle = np.loadtxt(\"3D_Experements/\"+exp_name+\"_z.txt\")\n",
    "\n",
    "    print(np.amax(upper_angle*180/np.pi))\n",
    "    print(np.amin(upper_angle*180/np.pi))\n",
    "    print(np.amax(lower_angle*180/np.pi))\n",
    "    print(np.amin(lower_angle*180/np.pi))\n",
    "    \n",
    "\n",
    "    #x , y , z = np.array(dist)*np.cos(uAngel)*np.sin(lAngel) , np.array(dist)*np.cos(uAngel)*np.cos(lAngel) , np.array(dist)*np.sin(uAngel)\n",
    "\n",
    "    my_sample_x = np.array(dist)*np.cos(upper_angle)*np.sin(lower_angle)\n",
    "    my_sample_y =  np.array(dist)*np.cos(upper_angle)*np.cos(lower_angle)\n",
    "    my_sample_z =  np.array(dist)*np.sin(upper_angle)\n",
    "\n",
    "    minx = np.min(my_sample_x)\n",
    "    miny = np.min(my_sample_y)\n",
    "    minz = np.min(my_sample_z)\n",
    "\n",
    "    maxx = np.max(my_sample_x)\n",
    "    maxy = np.max(my_sample_y)\n",
    "    maxz = np.max(my_sample_z)\n",
    "\n",
    "    # img =np.zeros(int(maxx-minx),int(maxz-minz))\n",
    "    # img =np.zeros((int(maxx-minx),int(maxz-minz)), dtype=np.float64)\n",
    "    # img =np.zeros(len(my_sample_x),len(my_sample_z)), dtype=np.float64)\n",
    "\n",
    "    # for i in range(len(my_sample_x) - 1):\n",
    "    #     img[int(my_sample_x[i] - minx -1)][int(my_sample_z[i] - minz -1)] = (my_sample_y[i] / maxy)*255\n",
    "    \n",
    "    trace = go.Surface(x = my_sample_x, y = my_sample_y, z =my_sample_z )\n",
    "    data = [trace]\n",
    "    layout = go.Layout(title = '3D Surface plot')\n",
    "    fig = go.Figure(data = data)\n",
    "    plot(fig)\n",
    "    offline.init_notebook_mode(connected=False)\n",
    "\n",
    "    df3 = {'x':[1, 2, 3, 4, 5],'y':[10, 20, 30, 40, 50],'z': [[5, 4, 3, 2, 1]]*5}\n",
    "    offline.iplot(dict(data=[go.Surface(x=df3['x'], y=df3['y'], z=df3['z'])]))\n",
    "\n",
    "def plot_var(threshold):\n",
    "    exp_p = [\"gedo_14_7\",\n",
    "             \"waleed_14_7\",\n",
    "             \"ta7a_14_7\",\n",
    "             \"nassar_14_7\"]\n",
    "\n",
    "    exp_n = [\"metal_dish\",\n",
    "            \"painting\"]\n",
    "\n",
    "    var_all = []\n",
    "    count = 0\n",
    "    color = []\n",
    "\n",
    "    for i in range(len(exp_p)):\n",
    "        for j in range(1000):\n",
    "            exp_name = exp_p[i] + \"_\" + str(j)\n",
    "            dist = np.loadtxt(\"3D_Experements/\"+exp_name+\"_x.txt\")\n",
    "            upper_angle = np.loadtxt(\"3D_Experements/\"+exp_name+\"_y.txt\")\n",
    "            lower_angle = np.loadtxt(\"3D_Experements/\"+exp_name+\"_z.txt\")\n",
    "            my_sample_y =  np.array(dist)*np.cos(upper_angle)*np.cos(lower_angle)\n",
    "\n",
    "            min_depth = np.amin(my_sample_y)\n",
    "            max_depth = (min_depth + threshold)\n",
    "            indx = my_sample_y <= (min_depth + threshold)\n",
    "            my_sample_y = my_sample_y[indx]\n",
    "            var_all.append(np.var(my_sample_y))\n",
    "            count+=1\n",
    "            color.append(0)\n",
    "            # print(count)\n",
    "\n",
    "    \n",
    "    for i in range(len(exp_n)):\n",
    "        for j in range(1000):\n",
    "            exp_name = exp_n[i] + \"_\" + str(j)\n",
    "            dist = np.loadtxt(\"flat_Experements/\"+exp_name+\"_x.txt\")\n",
    "            upper_angle = np.loadtxt(\"flat_Experements/\"+exp_name+\"_y.txt\")\n",
    "            lower_angle = np.loadtxt(\"flat_Experements/\"+exp_name+\"_z.txt\")\n",
    "            my_sample_y =  np.array(dist)*np.cos(upper_angle)*np.cos(lower_angle)\n",
    "            # var_all.append(np.var(my_sample_y))\n",
    "\n",
    "            min_depth = np.amin(my_sample_y)\n",
    "            max_depth = (min_depth + threshold)\n",
    "            indx = my_sample_y <= (min_depth + threshold)\n",
    "            my_sample_y = my_sample_y[indx]\n",
    "            var_all.append(np.var(my_sample_y))\n",
    "            color.append(1)\n",
    "            count+=1\n",
    "            # print(count)\n",
    "\n",
    "    x = [num for num in range(0, len(var_all), 1)]\n",
    "    # a = np.array([x,var_all])\n",
    "\n",
    "    categories = np.array(color)\n",
    "\n",
    "    colormap = np.array(['g', 'r'])\n",
    "\n",
    "    plt.scatter(np.array(x), np.array(var_all), s=100, c=colormap[categories])\n",
    "\n",
    "    # plt.savefig('ScatterClassPlot.png')\n",
    "    # plt.show()\n",
    "     \n",
    "    \n",
    "    # plt.plot(x, var_all,'o',color = color)\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('var')\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.plot()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "78f5414a2b82b24a0f75db7369e80ac59b30f4e57f7ac86c9d8f1053b13985a0"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
